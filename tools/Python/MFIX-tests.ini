[main]
testTopDir  = rt-MFIX-Exa
webTopDir   = rt-MFIX-Exa/web
workTopDir  = rt-MFIX-Exa/work
sourceDir   = ../..
refdataDir  = refdata

## suiteName is the name prepended to all output directories
suiteName = MFIX-Exa

purge_output = 1


#####################
## For CPU
#####################
##
## The following  should build and run on localhost.  (Running may be very slow compared to a HPC environment)
##
# job_manager = local
# cpus_per_task = 1
# cmakeSetupOpts = -GNinja -DMFIX_MPI=ON -DMFIX_OMP=OFF -DCMAKE_BUILD_TYPE=Release
# MPIcommand = mpiexec --mca btl self,vader,tcp -n @nprocs@ @command@ > @output@ 2> @error@
# Label =


#####################
## For Joule GPU
#####################
##
## Uncomment the following to run on Joule with CUDA
## Tested on Joule with environment modules:   module load anaconda/3.8 gnu/9.3.0 openmpi/4.0.4_gnu9.3 cmake/3.20.4 git cuda/11.3
##
# job_manager = slurm
# slurm_command = salloc
# ntasks_per_node = 2
# ntasks_per_socket = 1
## partition is the SLURM queue(partition) name; ignored unless job_manager == slurm
# partition = gpu

# cmakeSetupOpts = -GNinja -DMFIX_MPI=YES -DMFIX_OMP=NO -DCMAKE_BUILD_TYPE=Release -DMFIX_CSG=NO -DMFIX_HYPRE=NO -DAMReX_TINY_PROFILE=NO -DMFIX_GPU_BACKEND=CUDA -DAMReX_CUDA_ARCH="60" -DGPUS_PER_SOCKET=1 -DGPUS_PER_NODE=2
# MPIcommand = mpiexec -npersocket 1 --mca btl self,vader,tcp -n @nprocs@ @command@ > @output@ 2> @error@
# Label = CUDA


# MPIcommand should use the placeholders:
#   @host@ to indicate where to put the hostname to run on
#   @nprocs@ to indicate where to put the number of processors
#   @command@ to indicate where to put the command to run
#   @output@ to indicate stdout
#
# only tests with useMPI = 1 will run in parallel
# nprocs is problem dependent and specified in the individual problem
# sections.

reportActiveTestsOnly = 1

# Add "GO UP" link at the top of the web page?
goUpLink = 1

# email
sendEmailWhenFail = 0
emailTo =
emailBody = Check https://ccse.lbl.gov/pub/RegressionTesting/MFIX-Exa/


# individual problems follow

[fluidbed_cyl]
buildDir = benchmarks/rtl2/fluidbed_cyl
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
stSuccessString = Time spent in main
compareParticles = 1


[fluidbed_periodic]
buildDir = benchmarks/rtl2/fluidbed_periodic
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
stSuccessString = Time spent in main
runtime_params = mfix.max_step=3


[fluidbed_sq]
buildDir = benchmarks/rtl2/fluidbed_sq
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
stSuccessString = Time spent in main
runtime_params = mfix.max_step=3


# [hdpe_2inFB_2umf]
# buildDir = benchmarks/rtl2/hdpe_2inFB_2umf
# inputFile = inputs
# target = mfix
# dim = 3
# restartTest = 0
# useMPI = 1
# numprocs = 4
# useOMP = 1
# numthreads = 1
# compileTest = 0
# stSuccessString = Time spent in main
# runtime_params =


# [hdpe_disengaging_freeboard]
# buildDir = benchmarks/rtl2/hdpe_disengaging_freeboard
# inputFile = inputs
# target = mfix
# dim = 3
# restartTest = 0
# useMPI = 1
# numprocs = 4
# useOMP = 1
# numthreads = 1
# compileTest = 0
# stSuccessString = Time spent in main
# runtime_params =


[hopper]
buildDir = benchmarks/rtl2/hopper
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
doVis = 0
stSuccessString = Time spent in main
runtime_params = mfix.max_step=3


[riser_cyl]
buildDir = benchmarks/rtl2/riser_cyl
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
stSuccessString = Time spent in main
runtime_params = mfix.max_step=3


[riser_sq]
buildDir = benchmarks/rtl2/riser_sq
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
stSuccessString = Time spent in main
runtime_params = mfix.max_step=3

[mueller]
buildDir = benchmarks/rtl2/mueller
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
stSuccessString = Time spent in main
runtime_params = mfix.max_step=3

[sscpi]
buildDir = benchmarks/rtl2/sscpi
inputFile = inputs
target = mfix
dim = 3
restartTest = 0
useMPI = 1
numprocs = 4
useOMP = 1
numthreads = 1
compileTest = 0
stSuccessString = Time spent in main
runtime_params = mfix.max_step=3
